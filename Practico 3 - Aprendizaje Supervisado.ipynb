{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universidad Nacional de Córdoba - Facultad de Matemática, Astronomía, Física y Computación\n",
    "\n",
    "### Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones 2022\n",
    "Búsqueda y Recomendación para Textos Legales\n",
    "\n",
    "Mentor: Jorge E. Pérez Villella\n",
    "\n",
    "# Práctico Introducción al Aprendizaje Automático\n",
    "\n",
    "Integrantes:\n",
    "* Fernando Agustin Cardellino\n",
    "* Adrian Zelaya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivos:\n",
    "\n",
    "Probar distintos modelos de clasificación para evaluar la performance y la exactitud de predicción de cada modelo. \n",
    "\n",
    "* Utilizando el corpus normalizado en el práctico anterior, transformar el texto en vectores numéricos utilizando scikit-learn comparando los 3 modelos de vectorización. Explicar cada uno estos modelos.\n",
    "\n",
    "* Clasificar los documentos por fuero. Trabajaremos con los siguientes modelos de clasificación de la librería scikit-learn: Logistic Regresion, Naive Bayes y SVM. En cada modelo probar distintos hiperparámetros, generar la Matriz de Confusión y la Curva ROC. Explicar los resultados obtenidos.\n",
    "\n",
    "* Determinar y justificar cual es el modelo con mejor performance.\n",
    "\n",
    "* Predecir el fuero de un documento utilizando el mejor modelo.\n",
    "\n",
    "Opcional:\n",
    "\n",
    "* Opcional: Profundizar el tema de stop words y cómo generar uno propio.\n",
    "\n",
    "Fecha de Entrega: 29 de julio de 2022\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter, snowball\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "#import nltk\n",
    "#nltk.download('stopwords')  # Descomentar para bajar las stopwords\n",
    "# Ref: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos modelo de spacy y stopwords de NLTK\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "CURR_DIR = os.getcwd()  # Gets current directory\n",
    "STOPWORDS_ES = stopwords.words('spanish')\n",
    "BREAKPOINT=5  # None para analizar todos los documentos, sino un número para analizar hasta n documentos\n",
    "MAX_WORDS=1000\n",
    "IMG_NAME = \"legal-icon-png\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones que vamos a utilizar en el Práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(dirName, return_dir=False):\n",
    "    # create a list of file and sub directories\n",
    "    # names in the given directory\n",
    "    files = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for file in files:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, file)\n",
    "        # If entry is a directory then get the list of files in this directory\n",
    "        if os.path.isdir(fullPath):\n",
    "            if return_dir:\n",
    "                allFiles.append(fullPath.split(os.sep)[-1])\n",
    "            else:\n",
    "                allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "\n",
    "    return allFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones específicas del análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de preprocesamiento\n",
    "\n",
    "def get_tokens(files_path, fuero_name=None, breakpoint=None, object_type=None):\n",
    "    \"\"\"\n",
    "    Función que arma el corpus de palabras y las tokeniza, utilizando SpaCy.\n",
    "    Si `breakpoint` != None, dar el número de docs con el que se interrumpe la función\n",
    "    object_type: indica el tipo de objeto que devuelve, i.e. 'token'/None (SpaCy object `token`), \n",
    "    'word' (texto/string) or 'entity' (SpaCy object `entity`) \n",
    "    \"\"\"\n",
    "    def get_conditions(token, case):\n",
    "        if case == 'entity':\n",
    "            return True\n",
    "        else:\n",
    "            return token.is_alpha and not token.is_stop\n",
    "    \n",
    "    corpus = [] # En esta lista guardamos diccionarios con data de cada documento\n",
    "\n",
    "    tokens = [] # En esta lista guardamos los tokens de todo el documento\n",
    "\n",
    "    i = 0\n",
    "    for filename in getListOfFiles(files_path):\n",
    "        tokens_ = [] # En esta lista guardamos los tokens por documento\n",
    "        file_name = filename.split(os.sep)[-1]\n",
    "        # extraemos el nombre del fuero a partir de la carpeta en donde reside el documento:\n",
    "        fuero = fuero_name if fuero_name is not None else filename.split(os.sep)[-2]\n",
    "\n",
    "        with open(filename, encoding='utf-8') as file:\n",
    "            file_text = file.read()\n",
    "            # Tokenizamos el corpus\n",
    "            nlp_doc = nlp(file_text)            \n",
    "            \n",
    "            iterable = nlp_doc.ents if object_type == 'entity' else nlp_doc\n",
    "            \n",
    "            for token in iterable:\n",
    "                if get_conditions(token, case=object_type):\n",
    "                    # lematizamos y pasamos a minúscula (en modo 'palabras'/'word' solamente)\n",
    "                    aux_token = token.lemma_.lower() if object_type == 'word' else token\n",
    "                    \n",
    "                    # insertamos los tokens en la lista de todos los tokens\n",
    "                    tokens.append(\n",
    "                        aux_token\n",
    "                    )\n",
    "                    # insertamos los tokens en la lista de los tokens específicos al documento\n",
    "                    tokens_.append(\n",
    "                        aux_token\n",
    "                    )\n",
    "            # insertamos la data del documento a la lista reservada para tal fin\n",
    "            corpus.append({\n",
    "                'id': i, 'archivo': file_name, 'fuero': fuero, 'texto': file_text, 'texto_clean': tokens_\n",
    "            })\n",
    "        \n",
    "        i += 1            \n",
    "        # cortamos la ejecución de acuerdo al valor de quiebre (usado para testeo)\n",
    "        if breakpoint:\n",
    "            if i > breakpoint:\n",
    "                break\n",
    "    \n",
    "    return tokens, pd.DataFrame(corpus)\n",
    "\n",
    "\n",
    "def get_lemmas_stem_from_tokens(tokens, stemmer, language='spanish'):\n",
    "    aux_dict = {\n",
    "        'word': [token.lower_ for token in tokens],\n",
    "        'lemma': [token.lemma_.lower() for token in tokens],\n",
    "        'stem': [stemmer.stem(token.lower_) for token in tokens]\n",
    "    }\n",
    "    return pd.DataFrame(aux_dict)\n",
    "    \n",
    "    \n",
    "def get_conteo_palabras(palabras):\n",
    "    \"\"\"Función que genera un pandas DataFrame con la frecuencia de las palabras\n",
    "    \"\"\"\n",
    "    palabras_df = pd.DataFrame([{'palabra': str(x).lower()} for x in palabras])\n",
    "    # print(corpus_df.head())\n",
    "\n",
    "    return palabras_df.groupby(['palabra'])['palabra'].count().sort_values(ascending=False)\n",
    "\n",
    "    \n",
    "def generar_corpus(file_path, breakpoint, fuero_name=None, verbose=True, object_type='word'):\n",
    "    if verbose:\n",
    "        print(\"Generamos Corpus de palabras y conteo de frecuencias\")\n",
    "    \n",
    "    # Generamos el corpus de palabras, y el diccionario con el mapeo de los fueros con sus respectivos documentos\n",
    "    palabras, corpus = get_tokens(file_path, fuero_name=fuero_name, breakpoint=breakpoint, object_type='word')\n",
    "\n",
    "    frecuencia_palabras_df = get_conteo_palabras(palabras)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Algunas palabras: {palabras[:5]}\")\n",
    "    \n",
    "    return frecuencia_palabras_df, palabras, corpus\n",
    "\n",
    "\n",
    "def vectorize_corpus(vectorizer, corpus):\n",
    "    \"\"\"\n",
    "    Función que vectoriza un corpus de palabras compuesto por documentos, utilizando cualquier vectorizador de \n",
    "    scikit-learn que sea ingresado por el usuario.\n",
    "    Se asume que el corpus ya se encuentra tokenizado (mediante la función generar_corpus()), por lo que se 'anula'\n",
    "    el tokenizador por defecto de los vectorizadores.\n",
    "    \"\"\"\n",
    "    vectorizer_ = vectorizer(\n",
    "        tokenizer=lambda doc: doc, # Pisamos el tokenizador para que tome los tokens como vienen (ver descripción)\n",
    "        lowercase=False # Paso ya incluido en nuestro preprocesamiento\n",
    "    )\n",
    "    return vectorizer_.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubicación de los documentos\n",
    "filesDir = os.path.join(CURR_DIR, \"Documentos\")\n",
    "\n",
    "# Obtenemos lista de los fueros\n",
    "fueros = getListOfFiles(filesDir, return_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Análisis para 5 documentos\n",
      "\n",
      "palabra\n",
      "art       80\n",
      "luis      64\n",
      "ley       62\n",
      "vocal     62\n",
      "doctor    54\n",
      "Name: palabra, dtype: int64\n",
      "['sala', 'laboral', 'tribunal', 'superior', 'protocolo', 'sentencias', 'nº', 'resolución', 'año', 'tomo']\n",
      "   id                                            archivo    fuero  \\\n",
      "0   0                          9 BAEZ-FLECHA BUS.pdf.txt  LABORAL   \n",
      "1   1                            90 FUNES-COYSPU.pdf.txt  LABORAL   \n",
      "2   2                     1 QUINTEROS-CONSOLIDAR.pdf.txt  LABORAL   \n",
      "3   3  3 SANGUEDOLCE-MUNICIPALIDAD DE VILLA ALLENDE.p...  LABORAL   \n",
      "4   4                        188 LUCIANO-NICOLAS.pdf.txt  LABORAL   \n",
      "\n",
      "                                               texto  \\\n",
      "0  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
      "1  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
      "2  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
      "3  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
      "4  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
      "\n",
      "                                         texto_clean  \n",
      "0  [sala, laboral, tribunal, superior, protocolo,...  \n",
      "1  [sala, laboral, tribunal, superior, protocolo,...  \n",
      "2  [sala, laboral, tribunal, superior, protocolo,...  \n",
      "3  [sala, laboral, tribunal, superior, protocolo,...  \n",
      "4  [sala, laboral, tribunal, superior, protocolo,...  \n"
     ]
    }
   ],
   "source": [
    "n_docs = BREAKPOINT if not None else 'todos'\n",
    "    \n",
    "print(f\"\\nAnálisis para {n_docs} documentos\\n\")\n",
    "\n",
    "frecuencia_palabras_df, corpus, corpus_df = generar_corpus(filesDir, BREAKPOINT, object_type='word', verbose=False)\n",
    "\n",
    "print(frecuencia_palabras_df.head())\n",
    "\n",
    "print(corpus[:10])\n",
    "\n",
    "print(corpus_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4920\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformación del texto en vectores numéricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el corpus normalizado en el práctico anterior, transformar el texto en vectores numéricos utilizando scikit-learn comparando los 3 modelos de vectorización. Explicar cada uno estos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "\n",
    "Este vectorizador convierte una colección de textos en una matriz de recuentos de tokens.\n",
    "\n",
    "Para ello, primero **tokeniza** las palabras y asigna un token id (número entero) a cada token. El tokenizador por default utiliza espacios y separadores de puntuación como separadores. Luego, **cuenta** las ocurrencias de los tokens en cada documento.\n",
    "\n",
    "El producto de salida es una matriz rala/dispersa, donde cada fila representa un documento mientras que cada columna representa un token. Los valores de la matriz representan el número de ocurrencias del token en el documento.\n",
    "\n",
    "Referencias:\n",
    "1. [sckit-learn documentation - CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "2. [sckit-learn user guide - Text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0 ... 1 1 0]\n",
      " [0 0 1 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 2 0 ... 1 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " [0 0 3 ... 1 0 0]]\n",
      "(6, 1406)\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "\n",
    "X_1 = vectorize_corpus(CountVectorizer, corpus_df['texto_clean'])#corpus)\n",
    "\n",
    "print(X_1.toarray())\n",
    "print(X_1.toarray().shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer\n",
    "\n",
    "Este vectorizador convierte una colección de textos en una matriz de coeficientes **TF-IDF** (**T**erm-**F**requency - **I**nverse **D**ocument-**F**requency). Por la naturaleza del método, suele ser más útil en textos largos dado que para textos cortos los vectores resultantes pueden ser ruidosos/inestables. Es equivalente a utilizar el vectorizador `CountVectorizer` seguido del transformador `TfidfTransformer`.\n",
    "\n",
    "Los coeficientes TF-IDF nos permiten balancear el peso de las palabras de acuerdo a su frecuencia de ocurrencia, dándole menos importancia a palabras que se repiten seguido y resaltando aquellas que son más inusuales/raras.\n",
    "\n",
    "Para ello se aplica la siguiente transformación:\n",
    "\n",
    "$\\text{tf-idf}_{(t,d)} = \\text{tf}_{(t,d)} * \\text{idf}_{(t)}$ ,\n",
    "\n",
    "donde $t$ corresponde a un término (token/palabra) y $d$ a un documento, además:\n",
    "\n",
    "$\\text{tf}_{(t,d)} = \\frac{\\text{ocurrencias de } t \\text{ en } d }{\\text{número de palabras en }d}$ $ $ y $ $  $\\text{idf}_{(t)} = \\log{\\frac{1+n}{1+\\text{df}_{(t)}}}$ ,\n",
    "\n",
    "donde $n$ es el número total de documentos en el corpus y $\\text{df}$ es el número de documentos que contienen el término/palabra $t$.\n",
    "\n",
    "Finalmente, los vectores resultantes de cada documento, i.e. fila de la matriz, es normalizado utilizando la norma seleccionada (default: Euclideana o $L^2$):\n",
    "\n",
    "$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}}$\n",
    "\n",
    "Referencias:\n",
    "1. [sckit-learn documentation - TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "2. [sckit-learn user guide - Text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.04349111 0.         ... 0.01835909 0.02174556 0.        ]\n",
      " [0.         0.         0.022768   ... 0.         0.02696772 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.03487517]\n",
      " [0.         0.05075007 0.         ... 0.02142335 0.         0.        ]\n",
      " [0.03892785 0.         0.02695024 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.06214147 ... 0.02071382 0.         0.        ]]\n",
      "(6, 1406)\n"
     ]
    }
   ],
   "source": [
    "# ver normas L1 o L2 (default: L2)\n",
    "# ver de pasar los documentos y configurar los parámetros de preprocesamiento\n",
    "X_2 = vectorize_corpus(TfidfVectorizer, corpus_df['texto_clean'])\n",
    "\n",
    "\n",
    "print(X_2.toarray())\n",
    "print(X_2.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingVectorizer\n",
    "\n",
    "Este vectorizador convierte una colección de textos en una matriz de ocurrencias de tokens.\n",
    "\n",
    "Se diferencia de `CountVectorizer` en que utiliza *feature hashing*, i.e. aplica una función de hash a cada token y usa dicho hash como índice en vez de buscar los índices en alguna tabla asociativa.\n",
    "\n",
    "Esto hace que sea un método comparativamente mucho más rápido y con un uso reducido de memoria, útil para grandes datasets.\n",
    "\n",
    "En cuanto a sus desventajas, como no guarda registro de las características de los inputs originales, no es posible aplicar la transformación inversa, lo cual implica que no es posible saber qué características (features) pueden ser más relevantes en el modelo. \n",
    "\n",
    "Otro inconveniente con el método es que no permite balancear los tokens según su frecuencia de ocurrencia (IDF), aunque esto se puede suplir incluyendo un `TfidfTransformer` en el pipeline.\n",
    "\n",
    "Referencias:\n",
    "1. [sckit-learn documentation - HashingVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer)\n",
    "2. [sckit-learn user guide - Text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick) \n",
    "3. [Wikipedia - Feature hashing](https://en.wikipedia.org/wiki/Feature_hashing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 932)\t-0.10306601058361442\n",
      "  (0, 6495)\t0.0343553368612048\n",
      "  (0, 9176)\t0.0171776684306024\n",
      "  (0, 9520)\t0.0171776684306024\n",
      "  (0, 13227)\t-0.0171776684306024\n",
      "  (0, 16547)\t-0.0171776684306024\n",
      "  (0, 18775)\t0.0171776684306024\n",
      "  (0, 20797)\t-0.0171776684306024\n",
      "  (0, 21666)\t0.0343553368612048\n",
      "  (0, 23868)\t-0.0171776684306024\n",
      "  (0, 24958)\t0.0171776684306024\n",
      "  (0, 25133)\t-0.0171776684306024\n",
      "  (0, 25678)\t-0.0171776684306024\n",
      "  (0, 26115)\t-0.0171776684306024\n",
      "  (0, 26798)\t-0.0171776684306024\n",
      "  (0, 30298)\t-0.0171776684306024\n",
      "  (0, 33190)\t0.08588834215301201\n",
      "  (0, 48516)\t-0.15459901587542163\n",
      "  (0, 49032)\t-0.0171776684306024\n",
      "  (0, 50569)\t-0.0171776684306024\n",
      "  (0, 52575)\t-0.05153300529180721\n",
      "  (0, 54040)\t-0.05153300529180721\n",
      "  (0, 54733)\t-0.0171776684306024\n",
      "  (0, 57213)\t-0.0171776684306024\n",
      "  (0, 58260)\t0.0343553368612048\n",
      "  :\t:\n",
      "  (5, 988158)\t0.018847805889708434\n",
      "  (5, 989406)\t0.09423902944854216\n",
      "  (5, 993559)\t0.0565434176691253\n",
      "  (5, 999479)\t0.09423902944854216\n",
      "  (5, 1006857)\t0.018847805889708434\n",
      "  (5, 1007461)\t-0.03769561177941687\n",
      "  (5, 1009357)\t0.018847805889708434\n",
      "  (5, 1010077)\t-0.018847805889708434\n",
      "  (5, 1011117)\t0.03769561177941687\n",
      "  (5, 1013626)\t-0.018847805889708434\n",
      "  (5, 1016296)\t-0.018847805889708434\n",
      "  (5, 1016700)\t-0.018847805889708434\n",
      "  (5, 1018103)\t0.018847805889708434\n",
      "  (5, 1019657)\t-0.0565434176691253\n",
      "  (5, 1023814)\t0.03769561177941687\n",
      "  (5, 1029808)\t-0.018847805889708434\n",
      "  (5, 1030371)\t0.07539122355883374\n",
      "  (5, 1030691)\t-0.018847805889708434\n",
      "  (5, 1032436)\t0.03769561177941687\n",
      "  (5, 1032691)\t-0.09423902944854216\n",
      "  (5, 1033744)\t0.1130868353382506\n",
      "  (5, 1033938)\t-0.018847805889708434\n",
      "  (5, 1037536)\t-0.018847805889708434\n",
      "  (5, 1040386)\t-0.03769561177941687\n",
      "  (5, 1042039)\t0.03769561177941687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/PycharmProjects/Busqueda-y-Recomendacion-para-Textos-Legales-Mentoria-2022/venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# además de norm, ver n_features, alternate_sign\n",
    "# ver de pasar los documentos y configurar los parámetros de preprocesamiento\n",
    "X_3 = vectorize_corpus(HashingVectorizer, corpus_df['texto_clean'])\n",
    "print(X_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de documentos por fuero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>archivo</th>\n",
       "      <th>texto</th>\n",
       "      <th>texto_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>188 LUCIANO-NICOLAS.pdf.txt</td>\n",
       "      <td>SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...</td>\n",
       "      <td>[sala, laboral, tribunal, superior, protocolo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9 BAEZ-FLECHA BUS.pdf.txt</td>\n",
       "      <td>SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...</td>\n",
       "      <td>[sala, laboral, tribunal, superior, protocolo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>220 MURUA-PROVINCIA.pdf.txt</td>\n",
       "      <td>SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...</td>\n",
       "      <td>[sala, laboral, tribunal, superior, protocolo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3 SANGUEDOLCE-MUNICIPALIDAD DE VILLA ALLENDE.p...</td>\n",
       "      <td>SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...</td>\n",
       "      <td>[sala, laboral, tribunal, superior, protocolo,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            archivo  \\\n",
       "4   4                        188 LUCIANO-NICOLAS.pdf.txt   \n",
       "0   0                          9 BAEZ-FLECHA BUS.pdf.txt   \n",
       "5   5                        220 MURUA-PROVINCIA.pdf.txt   \n",
       "3   3  3 SANGUEDOLCE-MUNICIPALIDAD DE VILLA ALLENDE.p...   \n",
       "\n",
       "                                               texto  \\\n",
       "4  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
       "0  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
       "5  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
       "3  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
       "\n",
       "                                         texto_clean  \n",
       "4  [sala, laboral, tribunal, superior, protocolo,...  \n",
       "0  [sala, laboral, tribunal, superior, protocolo,...  \n",
       "5  [sala, laboral, tribunal, superior, protocolo,...  \n",
       "3  [sala, laboral, tribunal, superior, protocolo,...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REVISAR !!!!!!\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0):\n",
    "    model = LogisticRegression(C=_C).fit(X_tr, y_tr)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('Test Score with', description, 'features', score)\n",
    "    return model\n",
    "\n",
    "\n",
    "X = corpus_df[[x for x in corpus_df.columns if x != 'fuero']]\n",
    "y = corpus_df['fuero']\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
