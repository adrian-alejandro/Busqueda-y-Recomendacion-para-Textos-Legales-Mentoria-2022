{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universidad Nacional de Córdoba - Facultad de Matemática, Astronomía, Física y Computación\n",
    "\n",
    "### Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones 2022\n",
    "Búsqueda y Recomendación para Textos Legales\n",
    "\n",
    "Mentor: Jorge E. Pérez Villella\n",
    "\n",
    "# Práctico 3 - Embeddings\n",
    "\n",
    "Integrantes:\n",
    "* Fernando Agustin Cardellino\n",
    "* Adrián Zelaya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivos:\n",
    "\n",
    "Esta notebook se enfoca en los siguientes ejercicios de la mentoría:\n",
    "\n",
    "Práctico 3:\n",
    "* Utilizando el corpus normalizado en el práctico anterior, transformar el texto en vectores numéricos utilizando scikit-learn comparando los 3 modelos de vectorización. Explicar cada uno estos modelos.\n",
    "\n",
    "Práctico 4:\n",
    "* Realizar el proceso utilizando Gensim-Doc2Vec. Generar un input texto.\n",
    "\n",
    "Los vectores obtenidos en esta notebook serán reutilizados en el siguiente [repositorio](https://github.com/adrian-alejandro/autoML), para la optimización automática de hiperparámetros de modelos de ML en el marco de la materia AutoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import warnings\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "except:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos modelo de spacy y stopwords de NLTK\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "STOPWORDS_ES = stopwords.words('spanish')\n",
    "BREAKPOINT=None  # None para analizar todos los documentos, sino un número para analizar hasta n documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubicación de los documentos\n",
    "CURR_DIR = os.getcwd()  # Gets current directory\n",
    "embeddings_dir = os.path.join(CURR_DIR, \"embeddings\")\n",
    "files_dir = os.path.join(CURR_DIR, \"Documentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones que vamos a utilizar en el Práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_files(dirname, return_dir=False):\n",
    "    \"\"\"Returns a list of the files and subdirectories contained in a given input directory dirname\n",
    "    \"\"\"\n",
    "    files = os.listdir(dirname)\n",
    "    all_files = list()\n",
    "    # Iterate over all the entries\n",
    "    for file in files:\n",
    "        # Create full path\n",
    "        full_path = os.path.join(dirname, file)\n",
    "        # If entry is a directory then get the list of files in this directory\n",
    "        if os.path.isdir(full_path):\n",
    "            if return_dir:\n",
    "                all_files.append(full_path.split(os.sep)[-1])\n",
    "            else:\n",
    "                all_files = all_files + get_list_of_files(full_path)\n",
    "        else:\n",
    "            all_files.append(full_path)\n",
    "\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones específicas del análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de preprocesamiento\n",
    "\n",
    "def initialize_dataset(files, sample_size=None):\n",
    "    \"\"\"Función que partir de un directorio o lista de archivos inicializa el dataset en un dataframe \n",
    "    con la información básica del corpus: 'archivo', 'fuero', 'path'\n",
    "    \"\"\"\n",
    "    def get_filename(file_path):\n",
    "        # extrae el nombre del archivo\n",
    "        return file_path.split(os.sep)[-1]\n",
    "    \n",
    "    def get_fuero(file_path):\n",
    "        # extrae el nombre del fuero a partir de la carpeta en donde reside el documento\n",
    "        return file_path.split(os.sep)[-2]\n",
    "    \n",
    "    if os.path.isdir(files): # si el input is un directorio\n",
    "        list_of_files = get_list_of_files(files)\n",
    "    else:\n",
    "        list_of_files = files \n",
    "    \n",
    "    dataset = [{'archivo': get_filename(file), 'fuero': get_fuero(file), 'path': file} for file in list_of_files]\n",
    "    if sample_size:\n",
    "        try:\n",
    "            return pd.DataFrame(dataset).sample(n=sample_size)\n",
    "        except:\n",
    "            pass\n",
    "    return pd.DataFrame(dataset)\n",
    "\n",
    "def preprocess_dataset(dataset, encoding='utf-8', object_type='word'):\n",
    "    \"\"\"Función que realiza las siguientes tareas de preprocesamiento de texto dado un dataframe de input:\n",
    "    - extrae texto dada la ruta de un archivo\n",
    "    - transforma el texto en tokens\n",
    "    - limpia el texto: remueve no-palabras y stopwords, lematiza tokens y los transforma a minúsculas\n",
    "    \"\"\"\n",
    "    \n",
    "    REPLACEMENTS = [('á', 'a'), ('é', 'e'), ('í', 'i'), ('ó', 'o'), ('ú', 'u')]\n",
    "    \n",
    "    def get_text_from_file(file_path):\n",
    "        with open(file_path, encoding=encoding) as f:\n",
    "            return f.read()\n",
    "        \n",
    "    def extract_tokens(text, object_type=object_type):\n",
    "        nlp_doc = nlp(text)\n",
    "        return nlp_doc.ents if object_type == 'entity' else nlp_doc\n",
    "    \n",
    "    def preprocess_tokens(tokens):\n",
    "        \"\"\"Remueve tokens que no sean palabras ni stopwords, y los pasa a su versión lematizada y en minúsculas\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return [replace_tokens(token.lemma_.lower() , replacements=REPLACEMENTS)\n",
    "                    for token in tokens if token.is_alpha and not token.is_stop]\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def replace_tokens(token, replacements):\n",
    "        aux = token\n",
    "        for old, new in replacements:\n",
    "            aux = aux.replace(old, new)\n",
    "        return aux\n",
    "        \n",
    "    \n",
    "    # Get text from files\n",
    "    dataset['texto'] = dataset['path'].swifter.apply(lambda x: get_text_from_file(x))\n",
    "    \n",
    "    # Split text into tokens\n",
    "    dataset['tokens'] = dataset['texto'].swifter.apply(lambda x: extract_tokens(x))\n",
    "    \n",
    "    # Clean tokens\n",
    "    dataset['texto_clean'] = dataset['tokens'].swifter.apply(lambda x: preprocess_tokens(x))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def vectorize_corpus(vectorizer, corpus, to_array=True):\n",
    "    \"\"\"\n",
    "    Función que vectoriza un corpus de palabras compuesto por documentos, utilizando cualquier vectorizador de \n",
    "    scikit-learn que sea ingresado por el usuario.\n",
    "    Se asume que el corpus ya se encuentra tokenizado (mediante la función generar_corpus()), por lo que se 'anula'\n",
    "    el tokenizador por defecto de los vectorizadores.\n",
    "    \"\"\"\n",
    "    vectorizer_ = vectorizer(\n",
    "        tokenizer=lambda doc: doc, # Pisamos el tokenizador para que tome los tokens como vienen (ver descripción)\n",
    "        lowercase=False # Paso ya incluido en nuestro preprocesamiento\n",
    "    )\n",
    "    if to_array:\n",
    "        return vectorizer_.fit_transform(corpus).toarray()\n",
    "    return vectorizer_.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generamos el corpus y tokenizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Análisis para None documentos\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6f089c5b264827b4e421ff8941330c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79b41a450e742b1a515f403f54cfcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee77973993b49858be90e1459ab1555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>archivo</th>\n",
       "      <th>fuero</th>\n",
       "      <th>path</th>\n",
       "      <th>texto</th>\n",
       "      <th>tokens</th>\n",
       "      <th>texto_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9 BAEZ-FLECHA BUS.pdf.txt</td>\n",
       "      <td>LABORAL</td>\n",
       "      <td>/home/adrian/PycharmProjects/Busqueda-y-Recome...</td>\n",
       "      <td>SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...</td>\n",
       "      <td>(SALA, LABORAL, -, TRIBUNAL, SUPERIOR, \\n\\n, P...</td>\n",
       "      <td>[sala, laboral, tribunal, superior, protocolo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90 FUNES-COYSPU.pdf.txt</td>\n",
       "      <td>LABORAL</td>\n",
       "      <td>/home/adrian/PycharmProjects/Busqueda-y-Recome...</td>\n",
       "      <td>SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...</td>\n",
       "      <td>(SALA, LABORAL, -, TRIBUNAL, SUPERIOR, \\n\\n, P...</td>\n",
       "      <td>[sala, laboral, tribunal, superior, protocolo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 QUINTEROS-CONSOLIDAR.pdf.txt</td>\n",
       "      <td>LABORAL</td>\n",
       "      <td>/home/adrian/PycharmProjects/Busqueda-y-Recome...</td>\n",
       "      <td>SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...</td>\n",
       "      <td>(SALA, LABORAL, -, TRIBUNAL, SUPERIOR, \\n\\n, P...</td>\n",
       "      <td>[sala, laboral, tribunal, superior, protocolo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 SANGUEDOLCE-MUNICIPALIDAD DE VILLA ALLENDE.p...</td>\n",
       "      <td>LABORAL</td>\n",
       "      <td>/home/adrian/PycharmProjects/Busqueda-y-Recome...</td>\n",
       "      <td>SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...</td>\n",
       "      <td>(SALA, LABORAL, -, TRIBUNAL, SUPERIOR, \\n\\n, P...</td>\n",
       "      <td>[sala, laboral, tribunal, superior, protocolo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188 LUCIANO-NICOLAS.pdf.txt</td>\n",
       "      <td>LABORAL</td>\n",
       "      <td>/home/adrian/PycharmProjects/Busqueda-y-Recome...</td>\n",
       "      <td>SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...</td>\n",
       "      <td>(SALA, LABORAL, -, TRIBUNAL, SUPERIOR, \\n\\n, P...</td>\n",
       "      <td>[sala, laboral, tribunal, superior, protocolo,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             archivo    fuero  \\\n",
       "0                          9 BAEZ-FLECHA BUS.pdf.txt  LABORAL   \n",
       "1                            90 FUNES-COYSPU.pdf.txt  LABORAL   \n",
       "2                     1 QUINTEROS-CONSOLIDAR.pdf.txt  LABORAL   \n",
       "3  3 SANGUEDOLCE-MUNICIPALIDAD DE VILLA ALLENDE.p...  LABORAL   \n",
       "4                        188 LUCIANO-NICOLAS.pdf.txt  LABORAL   \n",
       "\n",
       "                                                path  \\\n",
       "0  /home/adrian/PycharmProjects/Busqueda-y-Recome...   \n",
       "1  /home/adrian/PycharmProjects/Busqueda-y-Recome...   \n",
       "2  /home/adrian/PycharmProjects/Busqueda-y-Recome...   \n",
       "3  /home/adrian/PycharmProjects/Busqueda-y-Recome...   \n",
       "4  /home/adrian/PycharmProjects/Busqueda-y-Recome...   \n",
       "\n",
       "                                               texto  \\\n",
       "0  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
       "1  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
       "2  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
       "3  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
       "4  SALA LABORAL - TRIBUNAL SUPERIOR\\n\\nProtocolo ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  (SALA, LABORAL, -, TRIBUNAL, SUPERIOR, \\n\\n, P...   \n",
       "1  (SALA, LABORAL, -, TRIBUNAL, SUPERIOR, \\n\\n, P...   \n",
       "2  (SALA, LABORAL, -, TRIBUNAL, SUPERIOR, \\n\\n, P...   \n",
       "3  (SALA, LABORAL, -, TRIBUNAL, SUPERIOR, \\n\\n, P...   \n",
       "4  (SALA, LABORAL, -, TRIBUNAL, SUPERIOR, \\n\\n, P...   \n",
       "\n",
       "                                         texto_clean  \n",
       "0  [sala, laboral, tribunal, superior, protocolo,...  \n",
       "1  [sala, laboral, tribunal, superior, protocolo,...  \n",
       "2  [sala, laboral, tribunal, superior, protocolo,...  \n",
       "3  [sala, laboral, tribunal, superior, protocolo,...  \n",
       "4  [sala, laboral, tribunal, superior, protocolo,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_docs = BREAKPOINT if not None else 'todos'\n",
    "    \n",
    "print(f\"\\nAnálisis para {n_docs} documentos\\n\")\n",
    "\n",
    "# Inicializamos dataset\n",
    "data = initialize_dataset(files_dir, sample_size=BREAKPOINT)\n",
    "\n",
    "# Preprocesamos el dataset\n",
    "corpus_df = preprocess_dataset(data)\n",
    "\n",
    "display(corpus_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformación del texto en vectores numéricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el corpus normalizado en el práctico anterior, transformar el texto en vectores numéricos utilizando scikit-learn comparando los 3 modelos de vectorización. Explicar cada uno estos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "\n",
    "Este vectorizador convierte una colección de textos en una matriz de recuentos de tokens.\n",
    "\n",
    "Para ello, primero **tokeniza** las palabras y asigna un token id (número entero) a cada token. El tokenizador por default utiliza espacios y separadores de puntuación como separadores. Luego, **cuenta** las ocurrencias de los tokens en cada documento.\n",
    "\n",
    "El producto de salida es una matriz rala/dispersa, donde cada fila representa un documento mientras que cada columna representa un token. Los valores de la matriz representan el número de ocurrencias del token en el documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_cv = vectorize_corpus(CountVectorizer, corpus_df['texto_clean'])\n",
    "\n",
    "np.save(\n",
    "    os.path.join(embeddings_dir, 'embedding_CV.npy'),\n",
    "    vector_cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer\n",
    "\n",
    "Este vectorizador convierte una colección de textos en una matriz de coeficientes **TF-IDF** (**T**erm-**F**requency - **I**nverse **D**ocument-**F**requency). Por la naturaleza del método, suele ser más útil en textos largos dado que para textos cortos los vectores resultantes pueden ser ruidosos/inestables. Es equivalente a utilizar el vectorizador `CountVectorizer` seguido del transformador `TfidfTransformer`.\n",
    "\n",
    "Los coeficientes TF-IDF nos permiten balancear el peso de las palabras de acuerdo a su frecuencia de ocurrencia, dándole menos importancia a palabras que se repiten seguido y resaltando aquellas que son más inusuales/raras.\n",
    "\n",
    "Para ello se aplica la siguiente transformación:\n",
    "\n",
    "$\\text{tf-idf}_{(t,d)} = \\text{tf}_{(t,d)} * \\text{idf}_{(t)}$ ,\n",
    "\n",
    "donde $t$ corresponde a un término (token/palabra) y $d$ a un documento, además:\n",
    "\n",
    "$\\text{tf}_{(t,d)} = \\frac{\\text{ocurrencias de } t \\text{ en } d }{\\text{número de palabras en }d}$ $ $ y $ $  $\\text{idf}_{(t)} = \\log{\\frac{1+n}{1+\\text{df}_{(t)}}}$ ,\n",
    "\n",
    "donde $n$ es el número total de documentos en el corpus y $\\text{df}$ es el número de documentos que contienen el término/palabra $t$.\n",
    "\n",
    "Finalmente, los vectores resultantes de cada documento, i.e. fila de la matriz, es normalizado utilizando la norma seleccionada (default: Euclideana o $L^2$):\n",
    "\n",
    "$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_tfidf = vectorize_corpus(TfidfVectorizer, corpus_df['texto_clean'])\n",
    "\n",
    "np.save(\n",
    "    os.path.join(embeddings_dir, 'embedding_TFIDF.npy'),\n",
    "    vector_tfidf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingVectorizer\n",
    "\n",
    "Este vectorizador convierte una colección de textos en una matriz de ocurrencias de tokens.\n",
    "\n",
    "Se diferencia de `CountVectorizer` en que utiliza *feature hashing*, i.e. aplica una función de hash a cada token y usa dicho hash como índice en vez de buscar los índices en alguna tabla asociativa.\n",
    "\n",
    "Esto hace que sea un método comparativamente mucho más rápido y con un uso reducido de memoria, útil para grandes datasets.\n",
    "\n",
    "En cuanto a sus desventajas, como no guarda registro de las características de los inputs originales, no es posible aplicar la transformación inversa, lo cual implica que no es posible saber qué características (features) pueden ser más relevantes en el modelo. \n",
    "\n",
    "Otro inconveniente con el método es que no permite balancear los tokens según su frecuencia de ocurrencia (IDF), aunque esto se puede suplir incluyendo un `TfidfTransformer` en el pipeline.\n",
    "\n",
    "Por otro lado, tokens distintos pueden ser mapeados al mismo índice (hash), aunque en la práctica esto rara vez ocurre dado que el número de atributos debe ser lo suficientemente grande, e.g. $2^{18}$ para problemas de clasificación de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_HV = vectorize_corpus(HashingVectorizer, corpus_df['texto_clean'], to_array=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vector_tfidf\n",
    "y = corpus_df.fuero.values\n",
    "\n",
    "X_path = os.path.join(embeddings_dir, 'X.npy')\n",
    "y_path = os.path.join(embeddings_dir, 'y.npy')\n",
    "df_path = os.path.join(embeddings_dir, 'processed_dataset.csv')\n",
    "\n",
    "np.save(\n",
    "    X_path,\n",
    "    X\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    y_path,\n",
    "    y\n",
    ")\n",
    "\n",
    "corpus_df.to_csv(\n",
    "    df_path,\n",
    "    index=False,\n",
    "    sep='|'    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipeamos para poder subir al repositorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: home/adrian/PycharmProjects/Busqueda-y-Recomendacion-para-Textos-Legales-Mentoria-2022/embeddings/X.npy (deflated 96%)\n",
      "  adding: home/adrian/PycharmProjects/Busqueda-y-Recomendacion-para-Textos-Legales-Mentoria-2022/embeddings/y.npy (deflated 78%)\n",
      "  adding: home/adrian/PycharmProjects/Busqueda-y-Recomendacion-para-Textos-Legales-Mentoria-2022/embeddings/processed_dataset.csv (deflated 75%)\n"
     ]
    }
   ],
   "source": [
    "for path in [X_path, y_path, df_path]:\n",
    "    zip_path = f\"{path}.zip\"\n",
    "    !zip -r {zip_path} {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [How to Encode Text Data for Machine Learning with scikit-learn](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)\n",
    "* [sckit-learn documentation - CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "* [sckit-learn user guide - Text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) \n",
    "* [sckit-learn documentation - TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "* [sckit-learn user guide - Text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) \n",
    "* [sckit-learn documentation - HashingVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer)\n",
    "* [sckit-learn user guide - Text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick) \n",
    "* [Wikipedia - Feature hashing](https://en.wikipedia.org/wiki/Feature_hashing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
